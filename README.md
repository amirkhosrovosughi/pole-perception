# pole-perception-yolo

Detect and classify utility poles from drone imagery using YOLO-based object detection models.

## üìÅ Project Structure
- `data/` ‚Äî raw and labeled images (not tracked)
- `notebooks/` ‚Äî Jupyter notebooks for training and validation
- `models/` ‚Äî saved YOLO weights
- `assets/` ‚Äî figures, demos, and visuals for documentation

## ‚öôÔ∏è Quick Start

Collect data:
<pre>
ros2 bag record /camera /fmu/out/vehicle_odometry
</pre>

Need to provide a metadata.xml file for needed information to extract the label. It would be something like:
```xml
<?xml version="1.0"?>
<scene>
  <!-- base -> camera pose as: x y z roll pitch yaw (radians) -->
  <base_to_camera>0.12 0.03 0.242 0 0.785 0</base_to_camera>

  <!-- pole position in world frame: x y z roll pitch yaw -->
  <!-- this should be world coordinates where you placed the pole in Gazebo -->
  <pole_position>2 0 -0.5 0 0 0</pole_position>

  <!-- camera intrinsics: fx fy cx cy (we'll parse K entry in your file) -->
  <camera_intrinsics>1393 1393 960 540</camera_intrinsics>

  <!-- pole physical height (meters) used to estimate bbox extent -->
  <pole_height>1.0</pole_height>
</scene>
```
Setup the environment

<pre>
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
pip install rosbags==0.9.21

python scripts/parse_bag_to_yolo.py \
  --bag bags/2025_11_30-12_30_33/rosbag2_2025_11_30-12_30_33 \
  --metadata bags/2025_11_30-12_30_33/metadata.xml \
  --output data/train \
  --topic-image /camera \
  --topic-odom /fmu/out/vehicle_odometry
</pre>


## Tool to validate data
This tool we subscribe to odom and image and show bounding box of the image, better to run in a seperate environment as it needs different depedencies version.
<pre>
source ros_venv/bin/activate
python src/bbox_from_odom_node.py
</pre>

## CVAT for Validating and Modifying Annotations
Use CVAT to visualize, validate, and edit your YOLO annotations.
Follow the official instructions for running CVAT locally on WSL:
https://docs.cvat.ai/docs/administration/basics/installation/?utm_source=chatgpt.com#windows-10

### Installation (One-Time Setup)
<pre>
git clone https://github.com/cvat-ai/cvat
cd cvat
CVAT_VERSION=dev docker compose up -d
</pre>

This launches the CVAT server in the background. After it is running, create an admin user:
<pre>
sudo docker exec -it cvat_server bash -ic 'python3 ~/manage.py createsuperuser'
</pre>

### Running CVAT

Once installed, CVAT starts automatically whenever Docker Desktop is running.
Simply open your browser and go to:
http://localhost:8080

Log in using the account you created.

### Loading Your Dataset

Use the annotation.zip file generated by parse_bag_to_yolo.py.

In CVAT:

1. Create a new Task
2. Upload your raw images (from WSL or a ZIP)
3. After the task is created, click ‚ÄúUpload Annotation‚Äù
4. Upload annotation.zip
‚Üí CVAT will load bounding boxes and labels. (sometimes need to Upload same file again with "Upload Annotations" so bounding box load correctly)

You can now inspect and modify the annotations visually.

### Exporting Corrected Annotations

After making changes:

1. Open your task
2. Click Export ‚Üí YOLO format (or CVAT-specific formats)
3. Replace/update your dataset with the exported labels.
