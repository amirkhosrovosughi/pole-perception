# pole-perception-yolo

Detect and classify utility poles from drone imagery using YOLO-based object detection models.

## üìÅ Project Structure
- `data/` ‚Äî raw and labeled images (not tracked)
- `notebooks/` ‚Äî Jupyter notebooks for training and validation
- `models/` ‚Äî saved YOLO weights
- `assets/` ‚Äî figures, demos, and visuals for documentation

## ‚öôÔ∏è Quick Start

This section walks through the steps required to **collect data and train the model**.  
This repository is **tightly integrated** with the main project:

üëâ https://github.com/amirkhosrovosughi/drone-adventure

The goal of this repository is to **train a deep learning model** that can be deployed in the above project.  
Data collection and validation are performed using the **Gazebo simulator** provided in that repository.

---

## üìä Data Collection (Analytical Labeling)

One approach to data collection is **analytical labeling**, where known **pole positions** and **robot odometry** are used to generate initial labeled data.

> ‚ö†Ô∏è The generated dataset will not be perfectly accurate, but it provides a **useful starting point** for bootstrapping the training process.

To collect this data, we record relevant topics from the **Gazebo simulator** into a **ROS bag file**.

The following topics should be stored:


<pre>
ros2 bag record /camera /fmu/out/vehicle_odometry
</pre>

Need to provide a metadata.xml file for needed information to extract the label. It would be something like:
```xml
<?xml version="1.0"?>
<scene>
  <!-- base -> camera pose as: x y z roll pitch yaw (radians) -->
  <base_to_camera>0.12 0.03 0.242 0 0.785 0</base_to_camera>

  <!-- pole position in world frame: x y z roll pitch yaw -->
  <!-- this should be world coordinates where you placed the pole in Gazebo -->
  <pole_position>2 0 -0.5 0 0 0</pole_position>

  <!-- camera intrinsics: fx fy cx cy (we'll parse K entry in your file) -->
  <camera_intrinsics>1393 1393 960 540</camera_intrinsics>

  <!-- pole physical height (meters) used to estimate bbox extent -->
  <pole_height>1.0</pole_height>
</scene>
```
Setup the environment

<pre>
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
pip install rosbags==0.9.21

python scripts/parse_bag_to_yolo.py \
  --bag bags/2025_12_27-10_34_22/rosbag2_2025_12_27-10_34_22 \
  --metadata bags/2025_12_27-10_34_22/metadata.xml \
  --output data/train_3_2025_12_27-10_34_22 \
  --topic-image /camera \
  --topic-odom /fmu/out/vehicle_odometry
  --frame_stride 10
</pre>


## Tool to validate data
This tool we subscribe to odom and image and show bounding box of the image, better to run in a seperate environment as it needs different depedencies version.
<pre>
source ros_venv/bin/activate
python src/bbox_from_odom_node.py
</pre>

## CVAT for Validating and Modifying Annotations
Use CVAT to visualize, validate, and edit your YOLO annotations.
Follow the official instructions for running CVAT locally on WSL:
https://docs.cvat.ai/docs/administration/basics/installation/?utm_source=chatgpt.com#windows-10

### Installation (One-Time Setup)
<pre>
git clone https://github.com/cvat-ai/cvat
cd cvat
CVAT_VERSION=dev docker compose up -d
</pre>

This launches the CVAT server in the background. After it is running, create an admin user:
<pre>
sudo docker exec -it cvat_server bash -ic 'python3 ~/manage.py createsuperuser'
</pre>

### Running CVAT

Once installed, CVAT starts automatically whenever Docker Desktop is running.
Simply open your browser and go to:
http://localhost:8080

Log in using the account you created.

### Loading Your Dataset

Use the annotation.zip file generated by parse_bag_to_yolo.py.

In CVAT:

1. Create a new Task
2. Upload your raw images (from WSL or a ZIP)
3. After the task is created, click ‚ÄúUpload Annotation‚Äù
4. Upload annotation.zip
‚Üí CVAT will load bounding boxes and labels. (sometimes need to Upload same file again with "Upload Annotations" so bounding box load correctly)

You can now inspect and modify the annotations visually.

### Exporting Corrected Annotations

After making changes:

1. Open your task
2. Click Export ‚Üí YOLO format (or CVAT-specific formats)
3. Replace/update your dataset with the exported labels.

# Training YOLO model
## Create a clean YOLO training environment
<pre>
python3 -m venv yolo_train_venv
source yolo_train_venv/bin/activate

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/<right_version_gpu>
pip install ultralytics opencv-python matplotlib jupyter tqdm
pip install pillow
pip install pyyaml
</pre>



Confirm that they are installed correctly
<pre>
yolo
python -c "import torch; print(torch.cuda.is_available())"
</pre>


## Import Data into YOLO Dataset

This script copies images and labels into a unified YOLO dataset and automatically renames files so numbering continues without collisions.

Usage
<pre>
cd ~/pole_perception
source yolo_train_venv/bin/activate
python scripts/import_dataset.py <source_path>
</pre>

Or with a custom validation split:
<pre>
source yolo_train_venv/bin/activate
python scripts/import_dataset.py <source_path> --val 0.25
</pre>

What it does
- nsures data/yolo_pole_dataset/images/train and labels/train exist
- Renames files to continuous IDs
- Copies images/labels into the YOLO dataset

## Train the model
For quick training and validation for terminal, you can run

<pre>
yolo train model=yolo11n.pt data=data/yolo_pole_dataset/dataset.yaml \
    imgsz=512 batch=4 epochs=100 mosaic=0 auto_augment=0 erasing=0
</pre>

To test the performance of model on the picture, you can try:
<pre>
yolo predict model=runs/detect/train3/weights/best.pt source=<path_to_image_or_folder>
</pre>

Or you can do the same with following the instructin on the Jupyter file train_yolo.ipyn.


## Convert the model to executable to use in ROS2 c++ node
<pre>
yolo export model=runs/detect/train3/weights/best.pt format=onnx opset=17
</pre>
This generates an ONNX model compatible with C++ inference in ROS2.

To verify the performance of this exported model, use this:

<pre>
yolo predict \
  model=/home/avosughi/labeled_data/model/best.onnx \
  source=~/temp_test/000094.jpg \
  device=cpu
<\pre>

# Side tools
## install and setup Jupyter
<pre>
source yolo_train_venv/bin/activate
pip install --upgrade pip
pip install jupyterlab ipykernel

# register this venv as a kernel (name and display name you can change)
python -m ipykernel install --user --name yolo_train --display-name "Python (yolo_train_venv)"

mkdir -p notebooks
touch notebooks/train_yolo.ipynb

# Runnig jupyter
# from ~/pole_perception and with your venv still activated
jupyter lab --notebook-dir=notebooks
</pre>

Follow the instruction to open the file, or copy + paste the provided link into your browser.

In the JupyterLab UI, click File ‚Üí New ‚Üí Notebook.
In the top-right kernel selector choose Python (yolo_train_venv) (the display name you set).
Save the notebook as train_yolo.ipynb inside notebooks/.


